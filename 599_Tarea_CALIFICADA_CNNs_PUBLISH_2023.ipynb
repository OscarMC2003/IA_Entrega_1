{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea calificada 1 INAR_2023\n",
    "\n",
    "\n",
    "## Cat vs. Dog Image Classification\n",
    "\n",
    "Este ejercicio consiste en desarrollar un modelo deep learning CNN (aunque eres libre de probar otras alternativas) para una dataset que contiene imágenes de perros y gatos (exclusivamente). \n",
    "\n",
    "Se debe entregar este notebook (como mínimo, se pueden entregar varios notebooks en un .zip) con los siguientes puntos:\n",
    "\n",
    "1. Carga y pre-proceso de ficheros de datos. (**YA DESARROLLADO, SOLO DEBE SER EJECUTADO**)\n",
    "2. Configura un (uno al menos, puedes presentar más) modelo CNN para la clasificación perro versus gato.\n",
    "3. Realiza el entrenamiento y prueba el rendimiento del modelo con el conjunto de test que hemos recopilado en clase este año.\n",
    "\n",
    "El dataset y la primera parte de este notebook están obtenidos, con infinitas gracias, de\n",
    "\n",
    "https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part1.ipynb\n",
    "\n",
    "Por tanto se debe mencionar la siguiente licencia:\n",
    "\n",
    "#### Copyright 2018 Google LLC.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
    "\n",
    "You may obtain a copy of the License at\n",
    "\n",
    " https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY6KJV6z6l7_"
   },
   "source": [
    "## 1. Carga y Pre-Proceso de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L7r2zdl64Hg"
   },
   "source": [
    "Descarga por favor el .zip \n",
    "\n",
    "cats_and_dogs_filtered.zip\n",
    "\n",
    "del Blackboard de la tarea en la misma ruta (carpeta) de este notebook y ejecuta los siguientes pasos.\n",
    "\n",
    "(o si usas Google Colab lo tienes en la dirección anterior)\n",
    "\n",
    "Mantengo la nota original del origen del dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keyquuYMWPMa"
   },
   "source": [
    "**NOTE:** The 2,000 images used in this exercise are excerpted from the [\"Dogs vs. Cats\" dataset](https://www.kaggle.com/c/dogs-vs-cats/data) available on Kaggle, which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLy3pthUS0D2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "local_zip = './dataset_tarea_1_2023.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-qUPyfO7Qr8"
   },
   "source": [
    "The contents of the .zip are extracted to the base directory `cats_and_dogs_filtered`, which contains `train` and `validation` subdirectories for the training and validation datasets (see the [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/validation/check-your-intuition) for a refresher on training, validation, and test sets), which in turn each contain `cats` and `dogs` subdirectories. \n",
    "\n",
    "NOTA: Además, en el mismo directorio donde se descomprime el cats_and_dogs_filtered descomprime el test.zip en un directorio test. \n",
    "\n",
    "Si todo está bien nombrado y \"colgando\" de la misma ruta, te saldrán los siguientes tamaños de conjuntos train     /validation / test (un poco más adelante)\n",
    "\n",
    "- total training cat images: 1000\n",
    "- total training dog images: 1000\n",
    "- total validation cat images: 500\n",
    "- total validation dog images: 500\n",
    "- total test cat images: 155\n",
    "- total test dog images: 158\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLZKVtE0dSfk"
   },
   "outputs": [],
   "source": [
    "base_dir = './dataset_tarea_1_2023'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "# Directory with our test cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "\n",
    "# Directory with our test dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuBYtA_Zd8_T"
   },
   "source": [
    "Now, let's see what the filenames look like in the `cats` and `dogs` `train` directories (file naming conventions are the same in the `validation` directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PIP1rkmeAYS"
   },
   "outputs": [],
   "source": [
    "train_cat_fnames = os.listdir(train_cats_dir)\n",
    "print(train_cat_fnames[:10])\n",
    "\n",
    "train_dog_fnames = os.listdir(train_dogs_dir)\n",
    "train_dog_fnames.sort()\n",
    "print(train_dog_fnames[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlqN5KbafhLI"
   },
   "source": [
    "Let's find out the total number of cat and dog images in the `train` and `validation` directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4XHh2xSfgie"
   },
   "outputs": [],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n",
    "print('total test cat images:', len(os.listdir(test_cats_dir)))\n",
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3WZABE9eX-8"
   },
   "source": [
    "For both cats and dogs, we have 1,000 training images and 500 test images.\n",
    "\n",
    "Now let's take a look at a few pictures to get a better sense of what the cat and dog datasets look like. First, configure the matplot parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2_Q0-_5UAv-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# Index for iterating over images\n",
    "pic_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTvHzGCxXkqp"
   },
   "source": [
    "Now, display a batch of 8 cat and 8 dog pictures. You can rerun the cell to see a fresh batch each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wpr8GxjOU8in"
   },
   "outputs": [],
   "source": [
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "\n",
    "pic_index += 8\n",
    "next_cat_pix = [os.path.join(train_cats_dir, fname) \n",
    "                for fname in train_cat_fnames[pic_index-8:pic_index]]\n",
    "next_dog_pix = [os.path.join(train_dogs_dir, fname) \n",
    "                for fname in train_dog_fnames[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
    "  # Set up subplot; subplot indices start at 1\n",
    "    sp = plt.subplot(nrows, ncols, i + 1)\n",
    "    sp.axis('Off') # Don't show axes (or gridlines)\n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn9m9D3UimHM"
   },
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Let's set up data generators that will read pictures in our source folders, convert them to `float32` tensors, and feed them (with their labels) to our network. We'll have one generator for the training images and one for the validation images. Our generators will yield batches of 20 images of size 150x150 and their labels (binary).\n",
    "\n",
    "As you may already know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network. (It is uncommon to feed raw pixels into a convnet.) In our case, we will preprocess our images by normalizing the pixel values to be in the `[0, 1]` range (originally all values are in the `[0, 255]` range).\n",
    "\n",
    "In Keras this can be done via the `keras.preprocessing.image.ImageDataGenerator` class using the `rescale` parameter. This `ImageDataGenerator` class allows you to instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`. These generators can then be used with the Keras model methods that accept data generators as inputs: `fit_generator`, `evaluate_generator`, and `predict_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClebU9NJg99G"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "base_dir = './dataset_tarea_1_2023'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False)\n",
    "\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "validation_generator = tf.keras.utils.image_dataset_from_directory(\n",
    "    validation_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False)\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "test_generator = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY6KJV6z6l7_"
   },
   "source": [
    "# 2. Modelo CNN (con summary + compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY6KJV6z6l7_"
   },
   "source": [
    "# 3. Entrenamiento del modelo (.fit) y rendimiento del modelo (accuracy) con conjunto test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Trabajo adicional (Neural Transfer, Fine Tuning, visualización de capas intermedias ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "# Cargar el modelo VGG16 preentrenado en ImageNet sin las capas densas (incluye las capas convolucionales)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "# Congelar las capas convolucionales\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Agregar tus propias capas densas para la clasificación binaria\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(2, activation='softmax')(x)  # Clasificación binaria, por lo que se utiliza 'softmax' con 2 neuronas\n",
    "\n",
    "# Crear el modelo completo\n",
    "model1 = Model(base_model.input, output_layer)\n",
    "\n",
    "\n",
    "# Resumen del modelo\n",
    "model1.summary()\n",
    "\n",
    "# Compilar el modelo\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Usar binary_crossentropy para clasificación binaria\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model1.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model1.evaluate(test_generator)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRUEBA MODELO CNN SECUENCIAL(BASICO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "\n",
    "# Obtener un lote de imágenes y etiquetas del generador\n",
    "for images, labels in test_generator.take(1):\n",
    "    break\n",
    "\n",
    "# Seleccionar un subconjunto de imágenes y etiquetas\n",
    "num_images_to_display = 5\n",
    "selected_images = images[:num_images_to_display]\n",
    "selected_labels = labels[:num_images_to_display]\n",
    "\n",
    "# Hacer predicciones\n",
    "predictions = model.predict(selected_images)\n",
    "\n",
    "# Mapeo de etiquetas (asegúrate de que coincida con tu dataset)\n",
    "class_names = ['Cat', 'Dog']\n",
    "\n",
    "# Visualizar las imágenes con las etiquetas predichas\n",
    "fig, axes = plt.subplots(num_images_to_display, 1, figsize=(12, num_images_to_display * 4))\n",
    "for i in range(num_images_to_display):\n",
    "    ax = axes[i]\n",
    "    # Convertir el tensor a un array de NumPy y cambiar el tipo de datos\n",
    "    image = selected_images[i].numpy().astype(\"uint8\")\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    actual_label = class_names[np.argmax(selected_labels[i])]\n",
    "    predicted_label = class_names[np.argmax(predictions[i])]\n",
    "    confidence = np.max(predictions[i])\n",
    "    ax.set_title(f\"Actual: {actual_label}, Predicted: {predicted_label} ({confidence:.2f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRUEBA MODELO CNN VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "\n",
    "# Obtener un lote de imágenes y etiquetas del generador\n",
    "for images, labels in test_generator.take(1):\n",
    "    break\n",
    "\n",
    "# Seleccionar un subconjunto de imágenes y etiquetas\n",
    "num_images_to_display = 5\n",
    "selected_images = images[:num_images_to_display]\n",
    "selected_labels = labels[:num_images_to_display]\n",
    "\n",
    "# Hacer predicciones\n",
    "predictions = model1.predict(selected_images)\n",
    "\n",
    "# Mapeo de etiquetas (asegúrate de que coincida con tu dataset)\n",
    "class_names = ['Cat', 'Dog']\n",
    "\n",
    "# Visualizar las imágenes con las etiquetas predichas\n",
    "fig, axes = plt.subplots(num_images_to_display, 1, figsize=(12, num_images_to_display * 4))\n",
    "for i in range(num_images_to_display):\n",
    "    ax = axes[i]\n",
    "    # Convertir el tensor a un array de NumPy y cambiar el tipo de datos\n",
    "    image = selected_images[i].numpy().astype(\"uint8\")\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    actual_label = class_names[np.argmax(selected_labels[i])]\n",
    "    predicted_label = class_names[np.argmax(predictions[i])]\n",
    "    confidence = np.max(predictions[i])\n",
    "    ax.set_title(f\"Actual: {actual_label}, Predicted: {predicted_label} ({confidence:.2f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelo_final_CNN_basico.h5\")\n",
    "model1.save(\"modelo_final_CNN_VGG.h5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qfja9tMaB1ZH"
   ],
   "name": "image_classification_part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
